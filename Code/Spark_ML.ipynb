{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# META DATA - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "    # Developer details: \n",
    "        # Name: Harish S\n",
    "        # Role: Architect\n",
    "        # Code ownership rights: Harish S\n",
    "    # Version:\n",
    "        # Version: V 1.0 (August 29th )\n",
    "            # Developer: Harish S\n",
    "     \n",
    "    # Description: The code enables to explore PySpark ML\n",
    "    \n",
    "# CODE - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
    "\n",
    "# Dependency: \n",
    "    # Environment:     \n",
    "        #Python 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT FOUNDATIONAL LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Library for numerical computation\n",
    "import pandas as pd # Library for data manipulation\n",
    "import os # system related operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INITIATE PYSPARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark # import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession,functions as F # initiate pyspark session to configure spark \n",
    "from pyspark.sql.types import IntegerType,StructType,StringType,LongType,FloatType,DoubleType # import datatypes to find the data based on its type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkSession.builder \\\n",
    "        .appName(\" spark ml \") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "READ THE DATA SOURCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = input(\"enter the path of source file: \") #datafile path\n",
    "main_df=sc.read.parquet(data_file_path+'combined_file.parquet') # customer data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- order_delivered_carrier_date: string (nullable = true)\n",
      " |-- order_delivered_customer_date: string (nullable = true)\n",
      " |-- order_estimated_delivery_date: string (nullable = true)\n",
      " |-- payment_sequential: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: string (nullable = true)\n",
      " |-- payment_value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.printSchema() #check datatypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREVIEW DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------------+-------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "|            order_id|         customer_id|  customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------------------+--------------------+--------------------+------------------------+-------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                   03149|    sao paulo|            SP|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|                 2|     voucher|                   1|        18.59|\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                   03149|    sao paulo|            SP|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|                 3|     voucher|                   1|         2.00|\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                   03149|    sao paulo|            SP|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|                 1| credit_card|                   1|        18.12|\n",
      "|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|af07308b275d755c9...|                   47813|    barreiras|            BA|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|                 1|      boleto|                   1|       141.46|\n",
      "|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|3a653a41f6f9fc3d2...|                   75265|   vianopolis|            GO|   delivered|     2018-08-08 08:38:49|2018-08-08 08:55:23|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|                 1| credit_card|                   3|       179.12|\n",
      "+--------------------+--------------------+--------------------+------------------------+-------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "main_df.show(5) # preview certain portion of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy of main dataframe\n",
    "df_copy=main_df.select(\"*\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (103886, 16)\n"
     ]
    }
   ],
   "source": [
    "# shape of the dataframe\n",
    "num_rows = df_copy.count() #no of rows\n",
    "num_columns = len(df_copy.columns) #no of cols\n",
    "print(f\"Shape: ({num_rows}, {num_columns})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ISOLATE CATEGORICAL AND NUMERICAL COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols=[field.name for field in df_copy.schema.fields if isinstance(field.dataType,StringType)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_types=[IntegerType,LongType,DoubleType,FloatType]\n",
    "Numerical_cols=[field.name for field in df_copy.schema.fields if isinstance(field.dataType,FloatType)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categorical_cols) #indicates no of categorical cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Numerical_cols) #indicates no of numerical cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "|order_id|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "|       0|          0|                 0|                       0|            0|             0|           0|                       0|              175|                        1888|                         3132|                            0|                 0|           0|                   0|            0|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_copy.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in categorical_cols]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as there 100000 rows, there are only 3000 rows at max where missing values are present hence its less than 10 %, hence deleting the rows with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_copy.dropna(subset=['order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "|order_id|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "|       0|          0|                 0|                       0|            0|             0|           0|                       0|                0|                           0|                            0|                            0|                 0|           0|                   0|            0|\n",
      "+--------+-----------+------------------+------------------------+-------------+--------------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in categorical_cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Encoding Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_index\") for c in categorical_cols]\n",
    "\n",
    "for indexer in indexers:\n",
    "    df_clean= indexer.fit(df_clean).transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:28:31 WARN DAGScheduler: Broadcasting large task binary with size 34.9 MiB\n",
      "24/09/02 20:28:32 WARN DAGScheduler: Broadcasting large task binary with size 34.9 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------------------------+-------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+--------------+-----------------+------------------------+------------------------------+-------------------+--------------------+------------------+------------------------------+-----------------------+----------------------------------+-----------------------------------+-----------------------------------+------------------------+------------------+--------------------------+-------------------+\n",
      "|            order_id|         customer_id|  customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|payment_sequential|payment_type|payment_installments|payment_value|order_id_index|customer_id_index|customer_unique_id_index|customer_zip_code_prefix_index|customer_city_index|customer_state_index|order_status_index|order_purchase_timestamp_index|order_approved_at_index|order_delivered_carrier_date_index|order_delivered_customer_date_index|order_estimated_delivery_date_index|payment_sequential_index|payment_type_index|payment_installments_index|payment_value_index|\n",
      "+--------------------+--------------------+--------------------+------------------------+-------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+--------------+-----------------+------------------------+------------------------------+-------------------+--------------------+------------------+------------------------------+-----------------------+----------------------------------+-----------------------------------+-----------------------------------+------------------------+------------------+--------------------------+-------------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                   03149|    sao paulo|            SP|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|                 2|     voucher|                   1|        18.59|         531.0|            461.0|                   281.0|                        5822.0|                0.0|                 0.0|               0.0|                         373.0|                  705.0|                            2207.0|                              390.0|                              288.0|                     1.0|               2.0|                       0.0|            11969.0|\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                   03149|    sao paulo|            SP|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|                 3|     voucher|                   1|         2.00|         531.0|            461.0|                   281.0|                        5822.0|                0.0|                 0.0|               0.0|                         373.0|                  705.0|                            2207.0|                              390.0|                              288.0|                     2.0|               2.0|                       0.0|             8637.0|\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|7c396fd4830fd0422...|                   03149|    sao paulo|            SP|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|                 1| credit_card|                   1|        18.12|         531.0|            461.0|                   281.0|                        5822.0|                0.0|                 0.0|               0.0|                         373.0|                  705.0|                            2207.0|                              390.0|                              288.0|                     0.0|               0.0|                       0.0|             8426.0|\n",
      "|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|af07308b275d755c9...|                   47813|    barreiras|            BA|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|                 1|      boleto|                   1|       141.46|       33433.0|          67571.0|                 65641.0|                       10931.0|              258.0|                 6.0|               0.0|                       88162.0|                 8595.0|                            1640.0|                            88920.0|                               26.0|                     0.0|               1.0|                       0.0|            11228.0|\n",
      "|47770eb9100c2d0c4...|41ce2a54c0b03bf34...|3a653a41f6f9fc3d2...|                   75265|   vianopolis|            GO|   delivered|     2018-08-08 08:38:49|2018-08-08 08:55:23|         2018-08-08 13:50:00|          2018-08-17 18:06:29|          2018-09-04 00:00:00|                 1| credit_card|                   3|       179.12|       28969.0|          26899.0|                 25607.0|                        9482.0|             2317.0|                 9.0|               0.0|                       91843.0|                84925.0|                            1716.0|                            91869.0|                              171.0|                     0.0|               0.0|                       2.0|             3654.0|\n",
      "+--------------------+--------------------+--------------------+------------------------+-------------+--------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+------------------+------------+--------------------+-------------+--------------+-----------------+------------------------+------------------------------+-------------------+--------------------+------------------+------------------------------+-----------------------+----------------------------------+-----------------------------------+-----------------------------------+------------------------+------------------+--------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clean.show(5) #preview the categorically converted data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Vector assembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PySpark, vector assembler converts all the rows and columns as a matrix which is further used for ML modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[c+\"_index\" for c in categorical_cols], outputCol=\"features\")\n",
    "df_clean = assembler.transform(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: string (nullable = true)\n",
      " |-- order_approved_at: string (nullable = true)\n",
      " |-- order_delivered_carrier_date: string (nullable = true)\n",
      " |-- order_delivered_customer_date: string (nullable = true)\n",
      " |-- order_estimated_delivery_date: string (nullable = true)\n",
      " |-- payment_sequential: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: string (nullable = true)\n",
      " |-- payment_value: string (nullable = true)\n",
      " |-- order_id_index: double (nullable = false)\n",
      " |-- customer_id_index: double (nullable = false)\n",
      " |-- customer_unique_id_index: double (nullable = false)\n",
      " |-- customer_zip_code_prefix_index: double (nullable = false)\n",
      " |-- customer_city_index: double (nullable = false)\n",
      " |-- customer_state_index: double (nullable = false)\n",
      " |-- order_status_index: double (nullable = false)\n",
      " |-- order_purchase_timestamp_index: double (nullable = false)\n",
      " |-- order_approved_at_index: double (nullable = false)\n",
      " |-- order_delivered_carrier_date_index: double (nullable = false)\n",
      " |-- order_delivered_customer_date_index: double (nullable = false)\n",
      " |-- order_estimated_delivery_date_index: double (nullable = false)\n",
      " |-- payment_sequential_index: double (nullable = false)\n",
      " |-- payment_type_index: double (nullable = false)\n",
      " |-- payment_installments_index: double (nullable = false)\n",
      " |-- payment_value_index: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original columns\n",
    "df_clean = df_clean.drop(*categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:31:44 WARN DAGScheduler: Broadcasting large task binary with size 34.9 MiB\n",
      "24/09/02 20:31:44 WARN DAGScheduler: Broadcasting large task binary with size 34.9 MiB\n",
      "[Stage 68:>                                                         (0 + 3) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+------------------------+------------------------------+-------------------+--------------------+------------------+------------------------------+-----------------------+----------------------------------+-----------------------------------+-----------------------------------+------------------------+------------------+--------------------------+-------------------+--------------------+\n",
      "|order_id_index|customer_id_index|customer_unique_id_index|customer_zip_code_prefix_index|customer_city_index|customer_state_index|order_status_index|order_purchase_timestamp_index|order_approved_at_index|order_delivered_carrier_date_index|order_delivered_customer_date_index|order_estimated_delivery_date_index|payment_sequential_index|payment_type_index|payment_installments_index|payment_value_index|            features|\n",
      "+--------------+-----------------+------------------------+------------------------------+-------------------+--------------------+------------------+------------------------------+-----------------------+----------------------------------+-----------------------------------+-----------------------------------+------------------------+------------------+--------------------------+-------------------+--------------------+\n",
      "|         531.0|            461.0|                   281.0|                        5822.0|                0.0|                 0.0|               0.0|                         373.0|                  705.0|                            2207.0|                              390.0|                              288.0|                     1.0|               2.0|                       0.0|            11969.0|[531.0,461.0,281....|\n",
      "|         531.0|            461.0|                   281.0|                        5822.0|                0.0|                 0.0|               0.0|                         373.0|                  705.0|                            2207.0|                              390.0|                              288.0|                     2.0|               2.0|                       0.0|             8637.0|[531.0,461.0,281....|\n",
      "|         531.0|            461.0|                   281.0|                        5822.0|                0.0|                 0.0|               0.0|                         373.0|                  705.0|                            2207.0|                              390.0|                              288.0|                     0.0|               0.0|                       0.0|             8426.0|[531.0,461.0,281....|\n",
      "|       33433.0|          67571.0|                 65641.0|                       10931.0|              258.0|                 6.0|               0.0|                       88162.0|                 8595.0|                            1640.0|                            88920.0|                               26.0|                     0.0|               1.0|                       0.0|            11228.0|[33433.0,67571.0,...|\n",
      "|       28969.0|          26899.0|                 25607.0|                        9482.0|             2317.0|                 9.0|               0.0|                       91843.0|                84925.0|                            1716.0|                            91869.0|                              171.0|                     0.0|               0.0|                       2.0|             3654.0|[28969.0,26899.0,...|\n",
      "+--------------+-----------------+------------------------+------------------------------+-------------------+--------------------+------------------+------------------------------+-----------------------+----------------------------------+-----------------------------------+-----------------------------------+------------------------+------------------+--------------------------+-------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_clean.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:39:50 WARN DAGScheduler: Broadcasting large task binary with size 34.9 MiB\n",
      "24/09/02 20:39:52 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/09/02 20:39:52 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:54 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:55 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:55 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:56 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:57 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:58 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:59 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:39:59 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:40:00 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:40:01 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:40:02 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:40:03 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:40:03 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "24/09/02 20:40:04 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"payment_type_index\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model on train data\n",
    "model = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:41:44 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.7436101838473419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/02 20:41:46 WARN DAGScheduler: Broadcasting large task binary with size 35.0 MiB\n",
      "[Stage 91:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set F1-score = 0.6399343823976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator #import multiclass evaluation class\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"payment_type_index\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions) # calculate metrics\n",
    "print(f\"Test set accuracy = {accuracy}\") # accuracy metrics\n",
    "f1 = MulticlassClassificationEvaluator(labelCol=\"payment_type_index\", predictionCol=\"prediction\", metricName=\"f1\").evaluate(predictions)\n",
    "print(f\"Test set F1-score = {f1}\") #f1 score metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
